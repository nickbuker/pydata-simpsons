{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dâ€™oh! Unevenly spaced time series analysis of The Simpsons in Pandas\n",
    "#### [PyData Seattle 2017 tutorial](https://pydata.org/seattle2017/schedule/presentation/104/)\n",
    "#### [Joe McCarthy](http://interrelativity.com/joe/), Data Scientist, [Indeed](https://www.indeed.com)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unevenly spaced opportunities to do unevenly spaced time series data analysis\n",
    "\n",
    "<p><a href=\"http://pandas.pydata.org/\"><img src=\"http://pandas.pydata.org/_static/pandas_logo.png\" style=\"margin: 0px 0px 10px 20px; width: 150px; float: right;\" title=\"pandas\" alt=\"pandas_logo.png\" /></a>I occasionally need to analyze gaps between events of interest, but not so often that I remember how to do it. I end up searching through my notebooks to remind myself which Pandas methods to use, and how to use them. I wanted to create a protypical example notebook that I can more easily reference .. and wanted to share it with others, in case they might also find it useful.</p>\n",
    "\n",
    "### Data.world\n",
    "\n",
    "<p><a href=\"https://data.world\"><img src=\"https://d2ogkq1rg66kh0.cloudfront.net/site-resources/images/fb-image-share.7b15f964.jpg\" style=\"margin: 0px 12px 5px 20px; width: 125px; float: right;\" title=\"data.world\" alt=\"data_world_logo.png\" /></a>The last time I hunted down a notebook containing unevenly spaced time series analysis was around the time I first heard about the [data.world](https://data.world) platform for creating, sharing and collaborating on data sets .. I think of it as a GitHub for data.</p>\n",
    "\n",
    "<p><a href=\"https://www.dataquest.io\"><img src=\"http://data-science-hack.com/wp-content/uploads/2016/04/dataquest-io-1-1.png\" style=\"margin: 0px 0px 5px 20px; width: 125px; float: right;\" title=\"dataquest.io\" alt=\"dataquest-io-logo.png\" /></a>I was further inspired about using data.world after reading a great tutorial created by Josh Devlin at [Dataquest.io](https://www.dataquest.io) on [Turbocharge Your Data Acquisition using the data.world Python Library](https://www.dataquest.io/blog/datadotworld-python-tutorial/). This is also where I discovered the dataset about The Simpsons episodes, which I think is more entertaining and generally accessible than the datasets I typically work with at Indeed (although those are interesting, too).</p>\n",
    "\n",
    "### PyData redemption\n",
    "\n",
    "I gave a tutorial on [Python for Data Science](https://pydata.org/seattle2015/schedule/presentation/8/) at PyData Seattle 2015, wherein I tried to squeeze a 4-hour tutorial into a 2-hour slot. It didn't work out so well, though I did make the [Jupyter Notebook from the tutorial](http://nbviewer.jupyter.org/github/gumption/Python_for_Data_Science/blob/master/Python_for_Data_Science_all.ipynb) publicly available, so attendees could go back over some of the things we rushed through or didn't even get to. \n",
    "\n",
    "For PyData Seattle 2017, I decided to propose a topic that I estimated would take more like an hour, so that we can take a somewhat more leisurely approach .. and, with any luck, we'll be able to get to lunch before the other morning tutorials let out. \n",
    "\n",
    "And I will be making this notebook publicly available as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python and Pandas in a Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the prerequisites for this tutorial are to have Python, Pandas and Jupyter Notebook installed on your computer. In case anyone has a computer that does not yet meet these requirements, I recommend downloading and installing [Anaconda](https://www.continuum.io/downloads), an open data science platform for Python, which includes Pandas, Jupyter Notebook and a variety of other useful open source Python libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python 2 vs. Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 major versions of Python in widespread use: [Python 2](https://docs.python.org/2/) and [Python 3](https://docs.python.org/3/). Python 3 has some features that are not backward compatible with Python 2, and some Python 2 libraries have not been updated to work with Python 3. I have been using Python 2, primarily because I use some of those Python 2[-only] libraries, but an increasing proportion of them are migrating to Python 3, and I anticipate shifting to Python 3 in the near future.\n",
    "\n",
    "For more on the topic, I recommend a very well documented IPython Notebook, which includes numerous helpful examples and links, by [Sebastian Raschka](http://sebastianraschka.com/), [Key differences between Python 2.7.x and Python 3.x](http://nbviewer.ipython.org/github/rasbt/python_reference/blob/master/tutorials/key_differences_between_python_2_and_3.ipynb), the [Cheat Sheet: Writing Python 2-3 compatible code](http://python-future.org/compatible_idioms.html) by Ed Schofield ... or [googling Python 2 vs 3](https://www.google.com/q=python%202%20vs%203).\n",
    "\n",
    "In order to make this notebook compatible with Python 2 *or* Python 3, we will import the [`print_function`]((https://docs.python.org/3/library/functions.html#print)) from the  [**`__future__`**](https://docs.python.org/2/library/__future__.html) module (in Python 2, `print` is a [statement](https://docs.python.org/2/reference/simple_stmts.html#print) not a function). We will also import the `division` module from the `future`, as I find [the use of `/` for \"true division\"](https://www.python.org/dev/peps/pep-0238/) - and the use of `//` for \"floor division\" - to be more aligned with my intuition.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also import - using the customary aliases - commonly used libraries in the python scientific stack, and specify that any plots will appear inline within the cell of this notebook (vs. popping up a separate window)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pandas\n",
    "\n",
    "This tutorial is designed for people who are already familiar with the `pandas` data analysis library.\n",
    "\n",
    "A thorough review of `pandas` concepts is beyond the scope of this session, but for participants who are not sufficiently familiar with `pandas`, I highly recommend the pandas [tutorial (\"10 Minutes to pandas\")](https://pandas.pydata.org/pandas-docs/stable/10min.html) and [Intro to Data Structures](https://pandas.pydata.org/pandas-docs/stable/dsintro.html).\n",
    "\n",
    "Here are a few basic concepts:\n",
    "\n",
    "* A [`DataFrame`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) is a tabular (2-dimensional) data structure with 2 axes: rows (`axis=0`) and columns (`axis=1`)\n",
    "* The columns in a `DataFrame` are one-dimensional arrays called [`Series`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.html)\n",
    "* Elements of a `DataFrame` can be accessed using labels, positions or boolean vectors\n",
    "  * The labels for a `Series` or the rows of a `DataFrame` are accessible via their `index` attribute\n",
    "  * The labels for each column in a `DataFrame` are accessible via its `columns` attribute\n",
    "  * Individual labels can be specified using brackets or - in most cases - dotted notation\n",
    "  * Positions can be specified using the [slice operators](https://docs.python.org/2/tutorial/introduction.html) for python sequences\n",
    "  \n",
    "Examples of these concepts will be highlighted when we first encounter them.\n",
    "\n",
    "Other important `pandas` concepts and operations used throughout this tutorial include [grouping (split-apply-combine)](http://pandas.pydata.org/pandas-docs/stable/groupby.html) and [merging](http://pandas.pydata.org/pandas-docs/stable/merging.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the data.world Python Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will borrow heavily - with permission - from Josh Devlin's tutorial, [Turbocharge Your Data Acquisition using the data.world Python library](https://www.dataquest.io/blog/datadotworld-python-tutorial/).\n",
    "\n",
    "There are additional elements incorporated from the [Python SDK documentation](https://data.world/nrippner/explore-the-data-world-python-sdk/workspace/file?filename=ddw_SDK.ipynb) the [Data Wrangling tutorial](https://data.world/nrippner/python-data-wrangling-tutorial/workspace/file?filename=datadotworld_wrangling_tutorial.ipynb) at data.world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing the data.world library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data.world Python SDK can be installed via the command line using the [`conda install` utility](https://conda.io/docs/using/pkgs.html#install-a-package-from-anaconda-org):\n",
    "\n",
    "`conda install -c conda-forge datadotworld-py`\n",
    "\n",
    "Alternately, you can use `pip` to install the SDK:\n",
    "\n",
    "`pip install git+git://github.com/datadotworld/data.world-py.git`\n",
    "\n",
    "Next, you need to retrieve and store your data.world API token.\n",
    "\n",
    "* From your data.world account, go to [Settings > Advanced](https://data.world/settings/advanced) and get your API token (the image below is from Josh's tutorial):\n",
    "\n",
    "<a href=\"https://data.world/settings/advanced\"> <img src=\"https://www.dataquest.io/blog/images/data.world_tutorial/api_token.png\" style=\"width: 600px;\" title=\"data.world Advanced Settings\" alt=\"data_world_api_token.png\" /></a>\n",
    "\n",
    "* If you installed the data.world Python SDK in a virtualenv or Conda env, activate that environment. \n",
    "* Run `dw configure` on the command line, which will prompt you for your token:\n",
    "```\n",
    "~ (datadotworld) $ dw configure\n",
    "API token (obtained at: https://data.world/settings/advanced): _\n",
    "```\n",
    "When you enter your token, a `~/.dw/` directory will created in your home directory and your token will be stored in your `~/.dw/config` file.\n",
    "\n",
    "Finally, if you used `pip install` above, you will need to run `pip install datadotworld[PANDAS]` on the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Simpsons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the benefit of anyone who is not familiar with The Simpsons, here is some context from the [Wikipedia page](https://en.wikipedia.org/wiki/The_Simpsons) for the television show:\n",
    "\n",
    "<blockquote>\n",
    "<p><a href=\"https://en.wikipedia.org/wiki/The_Simpsons\"><img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/0/0d/Simpsons_FamilyPicture.png/220px-Simpsons_FamilyPicture.png\" style=\"margin: 0px 0px 10px 20px; width: 150px; float: right;\" title=\"The Simpsons\" alt=\"Simpsons_FamilyPicture.png\" /></a>The Simpsons is an American animated sitcom created by Matt Groening for the Fox Broadcasting Company. The series is a satirical depiction of working-class life epitomized by the Simpson family, which consists of Homer, Marge, Bart, Lisa, and Maggie. The show is set in the fictional town of Springfield and parodies American culture, society, television, and the human condition.</p>\n",
    "\n",
    "<p>...</p>\n",
    "\n",
    "<p>Since its debut on December 17, 1989, 618 episodes of The Simpsons have been broadcast. Its 28th season began on September 25, 2016. It is the longest-running American sitcom and the longest-running American animated program, and, in 2009, it surpassed Gunsmoke as the longest-running American scripted primetime television series. The Simpsons Movie, a feature-length film, was released in theaters worldwide on July 27, 2007, and grossed over $527 million. On November 4, 2016, the series was renewed for a twenty-ninth and thirtieth season of 22 episodes each, extending the show to 2019.</p>\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Simpsons data set at data.world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Josh provides a brief history of this data set in [his tutorial](https://www.dataquest.io/blog/datadotworld-python-tutorial/):\n",
    "\n",
    "> For this tutorial, weâ€™ll be working with a data set of information on the TV show, [The Simpsons](https://en.wikipedia.org/wiki/The_Simpsons). The dataset was scraped by Tod Schenider for his post [The Simpsons by the Data](http://toddwschneider.com/posts/the-simpsons-by-the-data/), for which he made the scraper [available on GitHub](https://github.com/toddwschneider/flim-springfield). [Kaggle user William Cukierski used the scraper to upload the data set](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data), which was then [rehosted on data.world](https://data.world/data-society/the-simpsons-by-the-data).\n",
    "\n",
    "[The data set page on data.world](https://data.world/data-society/the-simpsons-by-the-data) shows four CSV files in the data set:\n",
    "\n",
    "* `simpsons_characters.csv`: Every character appearing in The Simpsons.\n",
    "* `simpsons_episodes.csv`: Every episode of the The Simpsons.\n",
    "* `simpsons_locations.csv`: Every location appearing in The Simpsons.\n",
    "* `simpsons_script_lines.csv`: Most lines from most scripts of the Simpsons.\n",
    "\n",
    "In _this_ tutorial, we'll only be using the data from `simpsons_episodes.csv` and `simpsons_script_lines.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using data.worldâ€™s Python library to explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with [Josh's tutorial](https://www.dataquest.io/blog/datadotworld-python-tutorial/):\n",
    "\n",
    "> First, letâ€™s import the `datadotworld` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datadotworld as dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Weâ€™re going to use the `load_dataset()` function to take a look at the data. When we use `load_dataset()` for the first time, it:\n",
    "> \n",
    "> * Downloads the data set from data.world and caches it in our `~/.dw/` directory\n",
    "> * Returns a `LocalDataset` object representing the data set\n",
    "> \n",
    "> Caching the data set locally is a really neat feature - it allows for quicker subsequent loading, lets you work on the data offline, ensures that your source data is the same each time you run your code, and in the future will support data set versioning. After the first time you call `load_dataset()` for a given dataset, it will load the dataset from the cached version. You can pass `True` to the optional `force_update` parameter if you wish to force a reload from the remote version and overwrite the changes.\n",
    "> \n",
    "> `load_dataset()` has one required parameter, `dataset_key` which you can extract from the URL of the data set on data.world. As an example, our simpsons data set has the URL https://data.world/data-society/the-simpsons-by-the-data, which makes its ID `data-society/the-simpsons-by-the-data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lds = dw.load_dataset('data-society/the-simpsons-by-the-data')  # , force_update=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the dataset, we can verify that it has been locally cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 70120\r\n",
      "-rw-r--r--  1 bqol  NORD\\Domain Users    222548 Jul  5 10:28 simpsons_characters.csv\r\n",
      "-rw-r--r--  1 bqol  NORD\\Domain Users    125999 Jul  5 10:28 simpsons_episodes.csv\r\n",
      "-rw-r--r--  1 bqol  NORD\\Domain Users    182776 Jul  5 10:28 simpsons_locations.csv\r\n",
      "-rw-r--r--  1 bqol  NORD\\Domain Users  35363962 Jul  5 10:28 simpsons_script_lines.csv\r\n"
     ]
    }
   ],
   "source": [
    "! ls -l ~/.dw/cache/data-society/the-simpsons-by-the-data/latest/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to know our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To take a closer look at our `LocalDataset` object , we can use the `LocalDataset.describe()` method, which returns a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'description': u'Contains the characters, locations, episode details, and script lines for approximately 600 Simpsons episodes.\\n\\nOriginally, this dataset was scraped by Tod Schenider for his post [The Simpsons by the Data](http://toddwschneider.com/posts/the-simpsons-by-the-data/), for which he made the scraper [available on GitHub](https://github.com/toddwschneider/flim-springfield). [Kaggle user William Cukierski used the scraper to upload the data set](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data), which has been rehosted here.\\n\\nAn additional column, `chalkboard_gag`, has been added to the `simpsons_episodes.csv` file, scraped from [Simpsons Wiki](http://simpsons.wikia.com/wiki/List_of_chalkboard_gags).',\n",
      " u'homepage': u'https://data.world/data-society/the-simpsons-by-the-data',\n",
      " u'keywords': [u'data society',\n",
      "               u'simpsons',\n",
      "               u'text mining',\n",
      "               u'characters',\n",
      "               u'script lines',\n",
      "               u'regression',\n",
      "               u'classification'],\n",
      " u'name': u'data-society_the-simpsons-by-the-data',\n",
      " u'resources': [{u'format': u'csv',\n",
      "                 u'name': u'simpsons_characters',\n",
      "                 u'path': u'data/simpsons_characters.csv'},\n",
      "                {u'format': u'csv',\n",
      "                 u'name': u'simpsons_episodes',\n",
      "                 u'path': u'data/simpsons_episodes.csv'},\n",
      "                {u'format': u'csv',\n",
      "                 u'name': u'simpsons_locations',\n",
      "                 u'path': u'data/simpsons_locations.csv'},\n",
      "                {u'format': u'csv',\n",
      "                 u'name': u'simpsons_script_lines',\n",
      "                 u'path': u'data/simpsons_script_lines.csv'},\n",
      "                {u'bytes': 214869,\n",
      "                 u'format': u'csv',\n",
      "                 u'mediatype': u'text/csv',\n",
      "                 u'name': u'original/simpsons_characters.csv',\n",
      "                 u'path': u'original/simpsons_characters.csv'},\n",
      "                {u'bytes': 125398,\n",
      "                 u'format': u'csv',\n",
      "                 u'mediatype': u'text/csv',\n",
      "                 u'name': u'original/simpsons_episodes.csv',\n",
      "                 u'path': u'original/simpsons_episodes.csv'},\n",
      "                {u'bytes': 178199,\n",
      "                 u'format': u'csv',\n",
      "                 u'mediatype': u'text/csv',\n",
      "                 u'name': u'original/simpsons_locations.csv',\n",
      "                 u'path': u'original/simpsons_locations.csv'},\n",
      "                {u'bytes': 35185146,\n",
      "                 u'format': u'csv',\n",
      "                 u'mediatype': u'text/csv',\n",
      "                 u'name': u'original/simpsons_script_lines.csv',\n",
      "                 u'path': u'original/simpsons_script_lines.csv'}],\n",
      " u'title': u'The Simpsons by the Data'}\n"
     ]
    }
   ],
   "source": [
    "# We use pprint as it makes our output easier to read \n",
    "import pprint as pp\n",
    "\n",
    "pp.pprint(lds.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON object has additional information since Josh created his tutorial, which has been inserted below:\n",
    "\n",
    "> Our JSON object has 6 key/value pairs at the top level: \n",
    "* `description`\n",
    "* `homepage` \n",
    "* `keywords` \n",
    "* `name` \n",
    "* `resouces` \n",
    "* `title`\n",
    "\n",
    "> `resources` is a list that contains information about each file in our data.world data set:\n",
    "* `name` \n",
    "* `format`\n",
    "* `path`\n",
    "* `mediatype`\n",
    "* `bytes`\n",
    "\n",
    "> Along with the `LocalDataset.describe()` function, there are three key attributes of our LocalDataset object which we can use to access the data itself:  \n",
    "* `LocalDataset.dataframes`\n",
    "* `LocalDataset.tables`\n",
    "* `LocalDataset.raw_data`\n",
    "\n",
    "> Each of these attributes work the same way, but return the data in a different format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'simpsons_episodes': LazyLoadedValue(<pandas.DataFrame>), u'simpsons_characters': LazyLoadedValue(<pandas.DataFrame>), u'simpsons_script_lines': LazyLoadedValue(<pandas.DataFrame>), u'simpsons_locations': LazyLoadedValue(<pandas.DataFrame>)} \n",
      "\n",
      "{u'simpsons_episodes': LazyLoadedValue(<list of rows>), u'simpsons_characters': LazyLoadedValue(<list of rows>), u'simpsons_script_lines': LazyLoadedValue(<list of rows>), u'simpsons_locations': LazyLoadedValue(<list of rows>)} \n",
      "\n",
      "{u'simpsons_episodes': LazyLoadedValue(<bytes>), u'original/simpsons_locations.csv': LazyLoadedValue(<bytes>), u'simpsons_characters': LazyLoadedValue(<bytes>), u'original/simpsons_episodes.csv': LazyLoadedValue(<bytes>), u'simpsons_locations': LazyLoadedValue(<bytes>), u'simpsons_script_lines': LazyLoadedValue(<bytes>), u'original/simpsons_script_lines.csv': LazyLoadedValue(<bytes>), u'original/simpsons_characters.csv': LazyLoadedValue(<bytes>)} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [lds.dataframes, lds.tables, lds.raw_data]:\n",
    "    print(i, '\\n')  # pprint does not workon lazy-loaded dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `LocalDataset.dataframes` returns a dictionary of pandas DataFrame objects, where as `LocalDataset.tables` and `LocalDataset.raw_data` returns the data in dictionaries of Python lists and bytes format respectively. Lists can be useful if we donâ€™t want to use pandas, and bytes is great if we have binary data like images or database files.\n",
    ">\n",
    "> Because of the power of the pandas library, letâ€™s use `LocalDataset.dataframes` to explore and have some fun with our data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis of The Simpsons data using `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.world provides a number of ways of working with data, including their own version of SQL (dwSQL), SPARQL and an R SDK (see their [Intro to data.world Dataset tutorial](https://data.world/jonloyens/an-intro-to-dataworld-dataset)), but in this tutorial we will focus only on using the Python SDK and using the `pandas DataFrames` in their datasets.\n",
    "\n",
    "We will create a `DataFrame` variable, `df`, to work with data from the `simpsons_script_lines` CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bqol/Envs/py27dev/lib/python2.7/site-packages/datadotworld/models/dataset.py:192: UserWarning: Unable to set data frame dtypes automatically using simpsons_script_lines schema. Data types may need to be adjusted manually. Error: Integer column has NA values in column 7\n",
      "  'Error: {}'.format(resource_name, e))\n",
      "/Users/bqol/Envs/py27dev/lib/python2.7/site-packages/datadotworld/util.py:136: DtypeWarning: Columns (4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return self._loader_func()\n"
     ]
    }
   ],
   "source": [
    "df = lds.dataframes['simpsons_script_lines']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warnings show that we have some columns with mixed types in this `DataFrame`.\n",
    "\n",
    "```\n",
    "/Users/joem/anaconda/lib/python2.7/site-packages/datadotworld/models/dataset.py:192: UserWarning: Unable to set data frame dtypes automatically using simpsons_script_lines schema. Data types may need to be adjusted manually. Error: Integer column has NA values in column 7\n",
    "  'Error: {}'.format(resource_name, e))\n",
    "/Users/joem/anaconda/lib/python2.7/site-packages/datadotworld/util.py:136: DtypeWarning: Columns (4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
    "  return self._loader_func()\n",
    "```\n",
    "\n",
    "We can use the [`pandas.DataFrame.info()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html) function to get more information on the contents of these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 158271 entries, 0 to 158270\n",
      "Data columns (total 13 columns):\n",
      "id                    158271 non-null int64\n",
      "episode_id            158271 non-null int64\n",
      "number                158271 non-null int64\n",
      "raw_text              158271 non-null object\n",
      "timestamp_in_ms       158271 non-null object\n",
      "speaking_line         158271 non-null object\n",
      "character_id          140750 non-null object\n",
      "location_id           157864 non-null float64\n",
      "raw_character_text    140749 non-null object\n",
      "raw_location_text     157863 non-null object\n",
      "spoken_words          132112 non-null object\n",
      "normalized_text       132087 non-null object\n",
      "word_count            132112 non-null object\n",
      "dtypes: float64(1), int64(3), object(9)\n",
      "memory usage: 15.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 columns have type `object`, which is usually an indicator that the column contains a mix of data types. \n",
    "\n",
    "Sometimes this happens when columns include null (`NaN`) values, so let's show the number of null values along with the number of data types, and include the number of unique values in each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: id:                  158271 unique values,      0 null values,  1 data type(s)\n",
      " 1: episode_id:             564 unique values,      0 null values,  1 data type(s)\n",
      " 2: number:                 395 unique values,      0 null values,  1 data type(s)\n",
      " 3: raw_text:            148280 unique values,      0 null values,  1 data type(s)\n",
      " 4: timestamp_in_ms:       2704 unique values,      0 null values,  2 data type(s)\n",
      " 5: speaking_line:            5 unique values,      0 null values,  2 data type(s)\n",
      " 6: character_id:          7185 unique values,  17521 null values,  2 data type(s)\n",
      " 7: location_id:           4459 unique values,    407 null values,  1 data type(s)\n",
      " 8: raw_character_text:    6765 unique values,  17522 null values,  2 data type(s)\n",
      " 9: raw_location_text:     4498 unique values,    408 null values,  2 data type(s)\n",
      "10: spoken_words:        122000 unique values,  26159 null values,  2 data type(s)\n",
      "11: normalized_text:     119804 unique values,  26184 null values,  2 data type(s)\n",
      "12: word_count:             114 unique values,  26159 null values,  2 data type(s)\n"
     ]
    }
   ],
   "source": [
    "for i, column_label in enumerate(df.columns):\n",
    "    print('{:2d}: {:20} {:6d} unique values, {:6d} null values, {:2d} data type(s)'.format(\n",
    "        i,\n",
    "        column_label + ':', \n",
    "        df[column_label].nunique(),\n",
    "        df[column_label].isnull().sum(),\n",
    "        len(df[column_label].apply(lambda x: type(x)).value_counts())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see what kinds of values are stored in the columns that have mixed data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 timestamp_in_ms\n",
      "<type 'int'>    131072\n",
      "<type 'str'>     27199\n",
      "Name: timestamp_in_ms, dtype: int64\n",
      "\n",
      "5 speaking_line\n",
      "<type 'bool'>    131072\n",
      "<type 'str'>      27199\n",
      "Name: speaking_line, dtype: int64\n",
      "\n",
      "6 character_id\n",
      "<type 'float'>    133921\n",
      "<type 'str'>       24350\n",
      "Name: character_id, dtype: int64\n",
      "\n",
      "8 raw_character_text\n",
      "<type 'str'>      140749\n",
      "<type 'float'>     17522\n",
      "Name: raw_character_text, dtype: int64\n",
      "\n",
      "9 raw_location_text\n",
      "<type 'str'>      157863\n",
      "<type 'float'>       408\n",
      "Name: raw_location_text, dtype: int64\n",
      "\n",
      "10 spoken_words\n",
      "<type 'str'>      132112\n",
      "<type 'float'>     26159\n",
      "Name: spoken_words, dtype: int64\n",
      "\n",
      "11 normalized_text\n",
      "<type 'str'>      132087\n",
      "<type 'float'>     26184\n",
      "Name: normalized_text, dtype: int64\n",
      "\n",
      "12 word_count\n",
      "<type 'str'>      132112\n",
      "<type 'float'>     26159\n",
      "Name: word_count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, column_label in enumerate(df.columns):\n",
    "    if len(df[column_label].apply(lambda x: type(x)).value_counts()) > 1:\n",
    "        print(i, column_label)\n",
    "        print(df[column_label].apply(lambda x: type(x)).value_counts())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of columns which have more than one data type, e.g., `str` and `float`. There are some other data quality issues in the `DataFrame` created from `simpsons_script_lines.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>timestamp_in_ms</th>\n",
       "      <th>speaking_line</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>raw_location_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9549</td>\n",
       "      <td>32</td>\n",
       "      <td>209</td>\n",
       "      <td>Miss Hoover: No, actually, it was a little of ...</td>\n",
       "      <td>848000</td>\n",
       "      <td>True</td>\n",
       "      <td>464</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "      <td>no actually it was a little of both sometimes ...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9550</td>\n",
       "      <td>32</td>\n",
       "      <td>210</td>\n",
       "      <td>Lisa Simpson: (NEAR TEARS) Where's Mr. Bergstrom?</td>\n",
       "      <td>856000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "      <td>wheres mr bergstrom</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9551</td>\n",
       "      <td>32</td>\n",
       "      <td>211</td>\n",
       "      <td>Miss Hoover: I don't know. Although I'd sure l...</td>\n",
       "      <td>856000</td>\n",
       "      <td>True</td>\n",
       "      <td>464</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "      <td>i dont know although id sure like to talk to h...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9552</td>\n",
       "      <td>32</td>\n",
       "      <td>212</td>\n",
       "      <td>Lisa Simpson: That life is worth living.</td>\n",
       "      <td>864000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>That life is worth living.</td>\n",
       "      <td>that life is worth living</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9553</td>\n",
       "      <td>32</td>\n",
       "      <td>213</td>\n",
       "      <td>Edna Krabappel-Flanders: The polls will be ope...</td>\n",
       "      <td>864000</td>\n",
       "      <td>True</td>\n",
       "      <td>40</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "      <td>the polls will be open from now until the end ...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  episode_id  number  \\\n",
       "0  9549          32     209   \n",
       "1  9550          32     210   \n",
       "2  9551          32     211   \n",
       "3  9552          32     212   \n",
       "4  9553          32     213   \n",
       "\n",
       "                                            raw_text timestamp_in_ms  \\\n",
       "0  Miss Hoover: No, actually, it was a little of ...          848000   \n",
       "1  Lisa Simpson: (NEAR TEARS) Where's Mr. Bergstrom?          856000   \n",
       "2  Miss Hoover: I don't know. Although I'd sure l...          856000   \n",
       "3           Lisa Simpson: That life is worth living.          864000   \n",
       "4  Edna Krabappel-Flanders: The polls will be ope...          864000   \n",
       "\n",
       "  speaking_line character_id  location_id       raw_character_text  \\\n",
       "0          True          464          3.0              Miss Hoover   \n",
       "1          True            9          3.0             Lisa Simpson   \n",
       "2          True          464          3.0              Miss Hoover   \n",
       "3          True            9          3.0             Lisa Simpson   \n",
       "4          True           40          3.0  Edna Krabappel-Flanders   \n",
       "\n",
       "               raw_location_text  \\\n",
       "0  Springfield Elementary School   \n",
       "1  Springfield Elementary School   \n",
       "2  Springfield Elementary School   \n",
       "3  Springfield Elementary School   \n",
       "4  Springfield Elementary School   \n",
       "\n",
       "                                        spoken_words  \\\n",
       "0  No, actually, it was a little of both. Sometim...   \n",
       "1                             Where's Mr. Bergstrom?   \n",
       "2  I don't know. Although I'd sure like to talk t...   \n",
       "3                         That life is worth living.   \n",
       "4  The polls will be open from now until the end ...   \n",
       "\n",
       "                                     normalized_text word_count  \n",
       "0  no actually it was a little of both sometimes ...         31  \n",
       "1                                wheres mr bergstrom          3  \n",
       "2  i dont know although id sure like to talk to h...         22  \n",
       "3                          that life is worth living          5  \n",
       "4  the polls will be open from now until the end ...         33  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another [notebook](pydata-tutorial-simpsons-data-cleaning.ipynb) in the repository contains a more detailed data analysis and cleaning of the `DataFrame`.\n",
    "\n",
    "For now, we'll just import a module with some utility functions and run a function to do the cleaning.\n",
    "\n",
    "If there is time - and interest - toward the end of the tutorial, we can review that notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bqol/Envs/py27dev/lib/python2.7/site-packages/pandas/core/indexing.py:477: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "import pydata_simpsons\n",
    "\n",
    "df = pydata_simpsons.clean_simpsons_script_lines(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>timestamp_in_ms</th>\n",
       "      <th>speaking_line</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>raw_location_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(Street: ext. street - establishing - night)</td>\n",
       "      <td>8000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>Street</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Car: int. car - night)</td>\n",
       "      <td>8000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>Car</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Marge Simpson: Ooo, careful, Homer.</td>\n",
       "      <td>8000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Marge Simpson</td>\n",
       "      <td>Car</td>\n",
       "      <td>Ooo, careful, Homer.</td>\n",
       "      <td>ooo careful homer</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Homer Simpson: There's no time to be careful.</td>\n",
       "      <td>10000</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>Car</td>\n",
       "      <td>There's no time to be careful.</td>\n",
       "      <td>theres no time to be careful</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Homer Simpson: We're late.</td>\n",
       "      <td>10000</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>Car</td>\n",
       "      <td>We're late.</td>\n",
       "      <td>were late</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  episode_id  number                                       raw_text  \\\n",
       "0   1           1       0   (Street: ext. street - establishing - night)   \n",
       "1   2           1       1                        (Car: int. car - night)   \n",
       "2   3           1       2            Marge Simpson: Ooo, careful, Homer.   \n",
       "3   4           1       3  Homer Simpson: There's no time to be careful.   \n",
       "4   5           1       4                     Homer Simpson: We're late.   \n",
       "\n",
       "   timestamp_in_ms speaking_line  character_id  location_id  \\\n",
       "0             8000         False             0            1   \n",
       "1             8000         False             0            2   \n",
       "2             8000          True             1            2   \n",
       "3            10000          True             2            2   \n",
       "4            10000          True             2            2   \n",
       "\n",
       "  raw_character_text raw_location_text                    spoken_words  \\\n",
       "0                               Street                                   \n",
       "1                                  Car                                   \n",
       "2      Marge Simpson               Car            Ooo, careful, Homer.   \n",
       "3      Homer Simpson               Car  There's no time to be careful.   \n",
       "4      Homer Simpson               Car                     We're late.   \n",
       "\n",
       "                normalized_text  word_count  \n",
       "0                                         0  \n",
       "1                                         0  \n",
       "2             ooo careful homer           3  \n",
       "3  theres no time to be careful           6  \n",
       "4                     were late           2  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use data from the `simpsons_episodes.csv` file to map episodes to seasons, so we will create a separate `DataFrame` for working with the episodes data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes = lds.dataframes['simpsons_episodes']\n",
    "len(df_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 600 entries, 0 to 599\n",
      "Data columns (total 14 columns):\n",
      "id                        600 non-null int64\n",
      "image_url                 596 non-null object\n",
      "imdb_rating               597 non-null float64\n",
      "imdb_votes                597 non-null float64\n",
      "number_in_season          600 non-null int64\n",
      "number_in_series          600 non-null int64\n",
      "original_air_date         600 non-null datetime64[ns]\n",
      "original_air_year         600 non-null int64\n",
      "production_code           600 non-null object\n",
      "season                    600 non-null int64\n",
      "title                     600 non-null object\n",
      "us_viewers_in_millions    594 non-null float64\n",
      "video_url                 596 non-null object\n",
      "views                     596 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(4), int64(5), object(4)\n",
      "memory usage: 65.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_episodes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image_url</th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>imdb_votes</th>\n",
       "      <th>number_in_season</th>\n",
       "      <th>number_in_series</th>\n",
       "      <th>original_air_date</th>\n",
       "      <th>original_air_year</th>\n",
       "      <th>production_code</th>\n",
       "      <th>season</th>\n",
       "      <th>title</th>\n",
       "      <th>us_viewers_in_millions</th>\n",
       "      <th>video_url</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1511.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1990-03-25</td>\n",
       "      <td>1990</td>\n",
       "      <td>7G10</td>\n",
       "      <td>1</td>\n",
       "      <td>Homer's Night Out</td>\n",
       "      <td>30.3</td>\n",
       "      <td>http://www.simpsonsworld.com/video/275197507879</td>\n",
       "      <td>50816.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1716.0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1990-04-29</td>\n",
       "      <td>1990</td>\n",
       "      <td>7G12</td>\n",
       "      <td>1</td>\n",
       "      <td>Krusty Gets Busted</td>\n",
       "      <td>30.4</td>\n",
       "      <td>http://www.simpsonsworld.com/video/288019523914</td>\n",
       "      <td>62561.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1638.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1990-10-11</td>\n",
       "      <td>1990</td>\n",
       "      <td>7F03</td>\n",
       "      <td>2</td>\n",
       "      <td>Bart Gets an \"F\"</td>\n",
       "      <td>33.6</td>\n",
       "      <td>http://www.simpsonsworld.com/video/260539459671</td>\n",
       "      <td>59575.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1457.0</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>1990-11-01</td>\n",
       "      <td>1990</td>\n",
       "      <td>7F01</td>\n",
       "      <td>2</td>\n",
       "      <td>Two Cars in Every Garage and Three Eyes on Eve...</td>\n",
       "      <td>26.1</td>\n",
       "      <td>http://www.simpsonsworld.com/video/260537411822</td>\n",
       "      <td>64959.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1366.0</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>1990-11-15</td>\n",
       "      <td>1990</td>\n",
       "      <td>7F08</td>\n",
       "      <td>2</td>\n",
       "      <td>Dead Putting Society</td>\n",
       "      <td>25.4</td>\n",
       "      <td>http://www.simpsonsworld.com/video/260539459670</td>\n",
       "      <td>50691.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          image_url  imdb_rating  \\\n",
       "0  10  http://static-media.fxx.com/img/FX_Networks_-_...          7.4   \n",
       "1  12  http://static-media.fxx.com/img/FX_Networks_-_...          8.3   \n",
       "2  14  http://static-media.fxx.com/img/FX_Networks_-_...          8.2   \n",
       "3  17  http://static-media.fxx.com/img/FX_Networks_-_...          8.1   \n",
       "4  19  http://static-media.fxx.com/img/FX_Networks_-_...          8.0   \n",
       "\n",
       "   imdb_votes  number_in_season  number_in_series original_air_date  \\\n",
       "0      1511.0                10                10        1990-03-25   \n",
       "1      1716.0                12                12        1990-04-29   \n",
       "2      1638.0                 1                14        1990-10-11   \n",
       "3      1457.0                 4                17        1990-11-01   \n",
       "4      1366.0                 6                19        1990-11-15   \n",
       "\n",
       "   original_air_year production_code  season  \\\n",
       "0               1990            7G10       1   \n",
       "1               1990            7G12       1   \n",
       "2               1990            7F03       2   \n",
       "3               1990            7F01       2   \n",
       "4               1990            7F08       2   \n",
       "\n",
       "                                               title  us_viewers_in_millions  \\\n",
       "0                                  Homer's Night Out                    30.3   \n",
       "1                                 Krusty Gets Busted                    30.4   \n",
       "2                                   Bart Gets an \"F\"                    33.6   \n",
       "3  Two Cars in Every Garage and Three Eyes on Eve...                    26.1   \n",
       "4                               Dead Putting Society                    25.4   \n",
       "\n",
       "                                         video_url    views  \n",
       "0  http://www.simpsonsworld.com/video/275197507879  50816.0  \n",
       "1  http://www.simpsonsworld.com/video/288019523914  62561.0  \n",
       "2  http://www.simpsonsworld.com/video/260539459671  59575.0  \n",
       "3  http://www.simpsonsworld.com/video/260537411822  64959.0  \n",
       "4  http://www.simpsonsworld.com/video/260539459670  50691.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only field we need from the episodes data is `season`, and we'll need the `id` field to map to the `episode_id` field in the `simpsons_script_lines` data, so we'll [merge](http://pandas.pydata.org/pandas-docs/stable/merging.html) those fields from `df_episodes` with `df` (using [`pandas.DataFrame.merge()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html)), and then drop the extraneous `id` column (`axis=1`) from `df_episodes` (using [`pandas.DataFrame.drop()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.merge(\n",
    "    df_episodes[['id', 'season']],\n",
    "    left_on='episode_id',\n",
    "    right_on='id',\n",
    "    how='left',\n",
    "    suffixes=['', '_y']).drop('id_y', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new `season` field will be in the rightmost column of the `DataFrame`, but I would like to see it appear right after the `id` field, so will re-order the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[[u'id', u'season', u'episode_id', u'number', u'raw_text', u'timestamp_in_ms',\n",
    "       u'speaking_line', u'character_id', u'location_id',\n",
    "       u'raw_character_text', u'raw_location_text', u'spoken_words',\n",
    "       u'normalized_text', u'word_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>season</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>timestamp_in_ms</th>\n",
       "      <th>speaking_line</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>raw_location_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(Street: ext. street - establishing - night)</td>\n",
       "      <td>8000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>Street</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Car: int. car - night)</td>\n",
       "      <td>8000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>Car</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Marge Simpson: Ooo, careful, Homer.</td>\n",
       "      <td>8000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Marge Simpson</td>\n",
       "      <td>Car</td>\n",
       "      <td>Ooo, careful, Homer.</td>\n",
       "      <td>ooo careful homer</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Homer Simpson: There's no time to be careful.</td>\n",
       "      <td>10000</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>Car</td>\n",
       "      <td>There's no time to be careful.</td>\n",
       "      <td>theres no time to be careful</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Homer Simpson: We're late.</td>\n",
       "      <td>10000</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>Car</td>\n",
       "      <td>We're late.</td>\n",
       "      <td>were late</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  season  episode_id  number  \\\n",
       "0   1       1           1       0   \n",
       "1   2       1           1       1   \n",
       "2   3       1           1       2   \n",
       "3   4       1           1       3   \n",
       "4   5       1           1       4   \n",
       "\n",
       "                                        raw_text  timestamp_in_ms  \\\n",
       "0   (Street: ext. street - establishing - night)             8000   \n",
       "1                        (Car: int. car - night)             8000   \n",
       "2            Marge Simpson: Ooo, careful, Homer.             8000   \n",
       "3  Homer Simpson: There's no time to be careful.            10000   \n",
       "4                     Homer Simpson: We're late.            10000   \n",
       "\n",
       "  speaking_line  character_id  location_id raw_character_text  \\\n",
       "0         False             0            1                      \n",
       "1         False             0            2                      \n",
       "2          True             1            2      Marge Simpson   \n",
       "3          True             2            2      Homer Simpson   \n",
       "4          True             2            2      Homer Simpson   \n",
       "\n",
       "  raw_location_text                    spoken_words  \\\n",
       "0            Street                                   \n",
       "1               Car                                   \n",
       "2               Car            Ooo, careful, Homer.   \n",
       "3               Car  There's no time to be careful.   \n",
       "4               Car                     We're late.   \n",
       "\n",
       "                normalized_text  word_count  \n",
       "0                                         0  \n",
       "1                                         0  \n",
       "2             ooo careful homer           3  \n",
       "3  theres no time to be careful           6  \n",
       "4                     were late           2  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Wikipedia](https://en.wikipedia.org/wiki/Time_series):\n",
    "\n",
    "> A **time series** is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.\n",
    ">\n",
    "> **Time series *analysis*** comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series data from The Simpsons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [The Simpsons dataset on data.world](https://data.world/data-society/the-simpsons-by-the-data), we will use two `DataFrames`, each loaded from its corresponding CSV file.\n",
    "\n",
    "Each data point (row) in the DataFrame we loaded from **`simpsons_script_lines.csv`** represents a distinct script line, indexed by an **`id`** field that indicates the order in which the line occured.\n",
    "\n",
    "Other fields in this `DataFrame` include\n",
    "* **`episode_id`**: an identifier for each episode\n",
    "* **`number`**: an identifier for the position of each line within an episode\n",
    "* **`raw_text`**: the raw text extracted from ???\n",
    "* **`timestamp_in_ms`**: the number of milliseconds since the start of the season when the line occurred\n",
    "* **`speaking_line`**: a boolean indicator of whether the line was a speaking line\n",
    "* **`character_id`**: a unique identifier for each character\n",
    "* **`location_id`**: a unique identifier for each location\n",
    "* **`raw_character_text`**: the full text (name) of the character\n",
    "* **`raw_location_text`**: the raw text (name) of the location\n",
    "* **`spoken_words`**: the text spoken in the line (if any)\n",
    "* **`normalized_text`**: a lowercase version of the `spoken_words`, with most punctuation removed\n",
    "* **`word_count`**: the number of space-delimited tokens in `normalized_text`\n",
    "\n",
    "Each data point in the `DataFrame` loaded from **`simpsons_episodes.csv`** represents a distinct episode, which is also indexed by an **`id`** field that indicates the order in which the episode occured. This `id` field maps to the `episode_id` in `simpsons_script_lines.csv`. The only other field from this `DataFrame` we will use is **`season`**, which is a number indicating the season in which the episode originally aired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that although there are 600 `id` values in `simpsons_episodes.csv`, there are only 564 `episode_id` values in `simpsons_script_lines.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 564, 36)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes.id.nunique(), df.episode_id.nunique(), len(df_episodes[~df_episodes.id.isin(df.episode_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image_url</th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>imdb_votes</th>\n",
       "      <th>number_in_season</th>\n",
       "      <th>number_in_series</th>\n",
       "      <th>original_air_date</th>\n",
       "      <th>original_air_year</th>\n",
       "      <th>production_code</th>\n",
       "      <th>season</th>\n",
       "      <th>title</th>\n",
       "      <th>us_viewers_in_millions</th>\n",
       "      <th>video_url</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>424</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>7.1</td>\n",
       "      <td>757.0</td>\n",
       "      <td>4</td>\n",
       "      <td>424</td>\n",
       "      <td>2008-11-02</td>\n",
       "      <td>2008</td>\n",
       "      <td>KABF16</td>\n",
       "      <td>20</td>\n",
       "      <td>Treehouse of Horror XIX</td>\n",
       "      <td>12.48</td>\n",
       "      <td>http://www.simpsonsworld.com/video/691895363987</td>\n",
       "      <td>21509.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>447</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.7</td>\n",
       "      <td>543.0</td>\n",
       "      <td>6</td>\n",
       "      <td>447</td>\n",
       "      <td>2009-11-22</td>\n",
       "      <td>2009</td>\n",
       "      <td>LABF18</td>\n",
       "      <td>21</td>\n",
       "      <td>Pranks and Greens</td>\n",
       "      <td>7.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>550</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>7.7</td>\n",
       "      <td>705.0</td>\n",
       "      <td>20</td>\n",
       "      <td>550</td>\n",
       "      <td>2014-05-04</td>\n",
       "      <td>2014</td>\n",
       "      <td>RABF21</td>\n",
       "      <td>25</td>\n",
       "      <td>Brick Like Me</td>\n",
       "      <td>4.39</td>\n",
       "      <td>http://www.simpsonsworld.com/video/311243331763</td>\n",
       "      <td>65613.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>570</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.7</td>\n",
       "      <td>334.0</td>\n",
       "      <td>18</td>\n",
       "      <td>570</td>\n",
       "      <td>2015-04-19</td>\n",
       "      <td>2015</td>\n",
       "      <td>TABF11</td>\n",
       "      <td>26</td>\n",
       "      <td>Peeping Mom</td>\n",
       "      <td>3.23</td>\n",
       "      <td>http://www.simpsonsworld.com/video/430426691868</td>\n",
       "      <td>40157.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>573</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.8</td>\n",
       "      <td>335.0</td>\n",
       "      <td>21</td>\n",
       "      <td>573</td>\n",
       "      <td>2015-05-10</td>\n",
       "      <td>2015</td>\n",
       "      <td>TABF15</td>\n",
       "      <td>26</td>\n",
       "      <td>Bull-E</td>\n",
       "      <td>2.77</td>\n",
       "      <td>http://www.simpsonsworld.com/video/442879555692</td>\n",
       "      <td>43978.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>576</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.7</td>\n",
       "      <td>395.0</td>\n",
       "      <td>2</td>\n",
       "      <td>576</td>\n",
       "      <td>2015-10-04</td>\n",
       "      <td>2015</td>\n",
       "      <td>TABF17</td>\n",
       "      <td>27</td>\n",
       "      <td>Cue Detective</td>\n",
       "      <td>6.02</td>\n",
       "      <td>http://www.simpsonsworld.com/video/770595395906</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>580</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.8</td>\n",
       "      <td>335.0</td>\n",
       "      <td>6</td>\n",
       "      <td>580</td>\n",
       "      <td>2015-11-08</td>\n",
       "      <td>2015</td>\n",
       "      <td>TABF21</td>\n",
       "      <td>27</td>\n",
       "      <td>Friend with Benefit</td>\n",
       "      <td>3.48</td>\n",
       "      <td>http://www.simpsonsworld.com/video/770437187884</td>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>583</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>8.4</td>\n",
       "      <td>698.0</td>\n",
       "      <td>9</td>\n",
       "      <td>583</td>\n",
       "      <td>2015-12-13</td>\n",
       "      <td>2015</td>\n",
       "      <td>VABF02</td>\n",
       "      <td>27</td>\n",
       "      <td>Barthood</td>\n",
       "      <td>5.97</td>\n",
       "      <td>http://www.simpsonsworld.com/video/770423875541</td>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>585</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.8</td>\n",
       "      <td>309.0</td>\n",
       "      <td>11</td>\n",
       "      <td>585</td>\n",
       "      <td>2016-01-10</td>\n",
       "      <td>2016</td>\n",
       "      <td>VABF04</td>\n",
       "      <td>27</td>\n",
       "      <td>Teenage Mutant Milk-Caused Hurdles</td>\n",
       "      <td>8.33</td>\n",
       "      <td>http://www.simpsonsworld.com/video/770430019813</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>598</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>598</td>\n",
       "      <td>2016-10-02</td>\n",
       "      <td>2016</td>\n",
       "      <td>VABF18</td>\n",
       "      <td>28</td>\n",
       "      <td>Friends and Family\"[203]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>599</td>\n",
       "      <td>2016-10-09</td>\n",
       "      <td>2016</td>\n",
       "      <td>VABF17</td>\n",
       "      <td>28</td>\n",
       "      <td>The Town\"[205]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>600</td>\n",
       "      <td>2016-10-16</td>\n",
       "      <td>2016</td>\n",
       "      <td>VABF16</td>\n",
       "      <td>28</td>\n",
       "      <td>Treehouse of Horror XXVII\"[207]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>588</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.6</td>\n",
       "      <td>293.0</td>\n",
       "      <td>14</td>\n",
       "      <td>588</td>\n",
       "      <td>2016-02-21</td>\n",
       "      <td>2016</td>\n",
       "      <td>VABF06</td>\n",
       "      <td>27</td>\n",
       "      <td>Gal of Constant Sorrow</td>\n",
       "      <td>3.10</td>\n",
       "      <td>http://www.simpsonsworld.com/video/770451523845</td>\n",
       "      <td>144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>590</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>7.3</td>\n",
       "      <td>262.0</td>\n",
       "      <td>16</td>\n",
       "      <td>590</td>\n",
       "      <td>2016-03-13</td>\n",
       "      <td>2016</td>\n",
       "      <td>VABF09</td>\n",
       "      <td>27</td>\n",
       "      <td>The Marge-ian Chronicles</td>\n",
       "      <td>3.07</td>\n",
       "      <td>http://www.simpsonsworld.com/video/770891331927</td>\n",
       "      <td>201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>593</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>7.1</td>\n",
       "      <td>229.0</td>\n",
       "      <td>19</td>\n",
       "      <td>593</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>2016</td>\n",
       "      <td>VABF12</td>\n",
       "      <td>27</td>\n",
       "      <td>Fland Canyon</td>\n",
       "      <td>2.77</td>\n",
       "      <td>http://www.simpsonsworld.com/video/770894403718</td>\n",
       "      <td>192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>595</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.4</td>\n",
       "      <td>227.0</td>\n",
       "      <td>21</td>\n",
       "      <td>595</td>\n",
       "      <td>2016-05-15</td>\n",
       "      <td>2016</td>\n",
       "      <td>VABF13</td>\n",
       "      <td>27</td>\n",
       "      <td>Simprovised</td>\n",
       "      <td>2.80</td>\n",
       "      <td>http://www.simpsonsworld.com/video/770892355976</td>\n",
       "      <td>228.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>579</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.8</td>\n",
       "      <td>385.0</td>\n",
       "      <td>5</td>\n",
       "      <td>579</td>\n",
       "      <td>2015-10-25</td>\n",
       "      <td>2015</td>\n",
       "      <td>TABF18</td>\n",
       "      <td>27</td>\n",
       "      <td>Treehouse of Horror XXVI</td>\n",
       "      <td>6.75</td>\n",
       "      <td>http://www.simpsonsworld.com/video/770433091920</td>\n",
       "      <td>195.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>584</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.6</td>\n",
       "      <td>356.0</td>\n",
       "      <td>10</td>\n",
       "      <td>584</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>2016</td>\n",
       "      <td>VABF03</td>\n",
       "      <td>27</td>\n",
       "      <td>The Girl Code</td>\n",
       "      <td>4.41</td>\n",
       "      <td>http://www.simpsonsworld.com/video/770434627559</td>\n",
       "      <td>166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>575</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>5.9</td>\n",
       "      <td>532.0</td>\n",
       "      <td>1</td>\n",
       "      <td>575</td>\n",
       "      <td>2015-09-27</td>\n",
       "      <td>2015</td>\n",
       "      <td>TABF14</td>\n",
       "      <td>27</td>\n",
       "      <td>Every Man's Dream</td>\n",
       "      <td>3.28</td>\n",
       "      <td>http://www.simpsonsworld.com/video/770426947744</td>\n",
       "      <td>187.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>572</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.2</td>\n",
       "      <td>330.0</td>\n",
       "      <td>20</td>\n",
       "      <td>572</td>\n",
       "      <td>2015-05-03</td>\n",
       "      <td>2015</td>\n",
       "      <td>TABF13</td>\n",
       "      <td>26</td>\n",
       "      <td>Let's Go Fly a Coot</td>\n",
       "      <td>3.12</td>\n",
       "      <td>http://www.simpsonsworld.com/video/438408259807</td>\n",
       "      <td>40783.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>569</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.7</td>\n",
       "      <td>374.0</td>\n",
       "      <td>17</td>\n",
       "      <td>569</td>\n",
       "      <td>2015-03-15</td>\n",
       "      <td>2015</td>\n",
       "      <td>TABF10</td>\n",
       "      <td>26</td>\n",
       "      <td>Waiting for Duffman</td>\n",
       "      <td>3.59</td>\n",
       "      <td>http://www.simpsonsworld.com/video/420981315819</td>\n",
       "      <td>37874.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>581</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.3</td>\n",
       "      <td>328.0</td>\n",
       "      <td>7</td>\n",
       "      <td>581</td>\n",
       "      <td>2015-11-22</td>\n",
       "      <td>2015</td>\n",
       "      <td>TABF20</td>\n",
       "      <td>27</td>\n",
       "      <td>Lisa with an 'S'</td>\n",
       "      <td>5.64</td>\n",
       "      <td>http://www.simpsonsworld.com/video/770417219938</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>587</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.3</td>\n",
       "      <td>252.0</td>\n",
       "      <td>13</td>\n",
       "      <td>587</td>\n",
       "      <td>2016-02-14</td>\n",
       "      <td>2016</td>\n",
       "      <td>VABF07</td>\n",
       "      <td>27</td>\n",
       "      <td>Love Is in the N2-O2-Ar-CO2-Ne-He-CH4</td>\n",
       "      <td>2.89</td>\n",
       "      <td>http://www.simpsonsworld.com/video/770439747712</td>\n",
       "      <td>158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>571</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.9</td>\n",
       "      <td>347.0</td>\n",
       "      <td>19</td>\n",
       "      <td>571</td>\n",
       "      <td>2015-04-26</td>\n",
       "      <td>2015</td>\n",
       "      <td>TABF12</td>\n",
       "      <td>26</td>\n",
       "      <td>The Kids Are All Fight</td>\n",
       "      <td>3.33</td>\n",
       "      <td>http://www.simpsonsworld.com/video/434682947703</td>\n",
       "      <td>42123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>586</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.7</td>\n",
       "      <td>271.0</td>\n",
       "      <td>12</td>\n",
       "      <td>586</td>\n",
       "      <td>2016-01-17</td>\n",
       "      <td>2016</td>\n",
       "      <td>VABF05</td>\n",
       "      <td>27</td>\n",
       "      <td>Much Apu About Something</td>\n",
       "      <td>3.95</td>\n",
       "      <td>http://www.simpsonsworld.com/video/770449475567</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>591</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>246.0</td>\n",
       "      <td>17</td>\n",
       "      <td>591</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>2016</td>\n",
       "      <td>VABF10</td>\n",
       "      <td>27</td>\n",
       "      <td>The Burns Cage</td>\n",
       "      <td>2.32</td>\n",
       "      <td>http://www.simpsonsworld.com/video/770893891557</td>\n",
       "      <td>190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>574</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.9</td>\n",
       "      <td>335.0</td>\n",
       "      <td>22</td>\n",
       "      <td>574</td>\n",
       "      <td>2015-05-17</td>\n",
       "      <td>2015</td>\n",
       "      <td>TABF16</td>\n",
       "      <td>26</td>\n",
       "      <td>Mathlete's Feat</td>\n",
       "      <td>2.82</td>\n",
       "      <td>http://www.simpsonsworld.com/video/446649411760</td>\n",
       "      <td>47429.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>589</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.3</td>\n",
       "      <td>233.0</td>\n",
       "      <td>15</td>\n",
       "      <td>589</td>\n",
       "      <td>2016-03-06</td>\n",
       "      <td>2016</td>\n",
       "      <td>VABF08</td>\n",
       "      <td>27</td>\n",
       "      <td>Lisa the Veterinarian</td>\n",
       "      <td>3.09</td>\n",
       "      <td>http://www.simpsonsworld.com/video/770452547696</td>\n",
       "      <td>174.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>582</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>7.1</td>\n",
       "      <td>305.0</td>\n",
       "      <td>8</td>\n",
       "      <td>582</td>\n",
       "      <td>2015-12-06</td>\n",
       "      <td>2015</td>\n",
       "      <td>VABF01</td>\n",
       "      <td>27</td>\n",
       "      <td>Paths of Glory</td>\n",
       "      <td>5.53</td>\n",
       "      <td>http://www.simpsonsworld.com/video/770422339536</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>577</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>7.2</td>\n",
       "      <td>404.0</td>\n",
       "      <td>3</td>\n",
       "      <td>577</td>\n",
       "      <td>2015-10-11</td>\n",
       "      <td>2015</td>\n",
       "      <td>TABF19</td>\n",
       "      <td>27</td>\n",
       "      <td>Puffless</td>\n",
       "      <td>3.31</td>\n",
       "      <td>http://www.simpsonsworld.com/video/772103747997</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>578</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>462.0</td>\n",
       "      <td>4</td>\n",
       "      <td>578</td>\n",
       "      <td>2015-10-18</td>\n",
       "      <td>2015</td>\n",
       "      <td>TABF22</td>\n",
       "      <td>27</td>\n",
       "      <td>Halloween of Horror</td>\n",
       "      <td>3.69</td>\n",
       "      <td>http://www.simpsonsworld.com/video/770425923947</td>\n",
       "      <td>196.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>594</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.7</td>\n",
       "      <td>193.0</td>\n",
       "      <td>20</td>\n",
       "      <td>594</td>\n",
       "      <td>2016-05-08</td>\n",
       "      <td>2016</td>\n",
       "      <td>VABF14</td>\n",
       "      <td>27</td>\n",
       "      <td>To Courier with Love</td>\n",
       "      <td>2.52</td>\n",
       "      <td>http://www.simpsonsworld.com/video/772102211921</td>\n",
       "      <td>192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>441</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>7.2</td>\n",
       "      <td>648.0</td>\n",
       "      <td>21</td>\n",
       "      <td>441</td>\n",
       "      <td>2009-05-17</td>\n",
       "      <td>2009</td>\n",
       "      <td>LABF12</td>\n",
       "      <td>20</td>\n",
       "      <td>Coming to Homerica</td>\n",
       "      <td>5.86</td>\n",
       "      <td>http://www.simpsonsworld.com/video/729848899646</td>\n",
       "      <td>6477.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>592</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.4</td>\n",
       "      <td>228.0</td>\n",
       "      <td>18</td>\n",
       "      <td>592</td>\n",
       "      <td>2016-04-10</td>\n",
       "      <td>2016</td>\n",
       "      <td>VABF11</td>\n",
       "      <td>27</td>\n",
       "      <td>How Lisa Got Her Marge Back</td>\n",
       "      <td>2.55</td>\n",
       "      <td>http://www.simpsonsworld.com/video/770893891750</td>\n",
       "      <td>188.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>596</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.9</td>\n",
       "      <td>210.0</td>\n",
       "      <td>22</td>\n",
       "      <td>596</td>\n",
       "      <td>2016-05-22</td>\n",
       "      <td>2016</td>\n",
       "      <td>VABF15</td>\n",
       "      <td>27</td>\n",
       "      <td>Orange Is the New Yellow</td>\n",
       "      <td>2.54</td>\n",
       "      <td>http://www.simpsonsworld.com/video/772111939897</td>\n",
       "      <td>276.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>597</td>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_...</td>\n",
       "      <td>6.6</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1</td>\n",
       "      <td>597</td>\n",
       "      <td>2016-09-25</td>\n",
       "      <td>2016</td>\n",
       "      <td>VABF20</td>\n",
       "      <td>28</td>\n",
       "      <td>Monty Burns' Fleeing Circus</td>\n",
       "      <td>3.36</td>\n",
       "      <td>http://www.simpsonsworld.com/video/772654659902</td>\n",
       "      <td>994.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                          image_url  imdb_rating  \\\n",
       "161  424  http://static-media.fxx.com/img/FX_Networks_-_...          7.1   \n",
       "189  447                                                NaN          6.7   \n",
       "209  550  http://static-media.fxx.com/img/FX_Networks_-_...          7.7   \n",
       "216  570  http://static-media.fxx.com/img/FX_Networks_-_...          6.7   \n",
       "229  573  http://static-media.fxx.com/img/FX_Networks_-_...          6.8   \n",
       "230  576  http://static-media.fxx.com/img/FX_Networks_-_...          6.7   \n",
       "231  580  http://static-media.fxx.com/img/FX_Networks_-_...          6.8   \n",
       "232  583  http://static-media.fxx.com/img/FX_Networks_-_...          8.4   \n",
       "233  585  http://static-media.fxx.com/img/FX_Networks_-_...          6.8   \n",
       "234  598                                                NaN          NaN   \n",
       "235  599                                                NaN          NaN   \n",
       "236  600                                                NaN          NaN   \n",
       "237  588  http://static-media.fxx.com/img/FX_Networks_-_...          6.6   \n",
       "238  590  http://static-media.fxx.com/img/FX_Networks_-_...          7.3   \n",
       "239  593  http://static-media.fxx.com/img/FX_Networks_-_...          7.1   \n",
       "240  595  http://static-media.fxx.com/img/FX_Networks_-_...          6.4   \n",
       "501  579  http://static-media.fxx.com/img/FX_Networks_-_...          6.8   \n",
       "502  584  http://static-media.fxx.com/img/FX_Networks_-_...          6.6   \n",
       "504  575  http://static-media.fxx.com/img/FX_Networks_-_...          5.9   \n",
       "505  572  http://static-media.fxx.com/img/FX_Networks_-_...          6.2   \n",
       "506  569  http://static-media.fxx.com/img/FX_Networks_-_...          6.7   \n",
       "507  581  http://static-media.fxx.com/img/FX_Networks_-_...          6.3   \n",
       "509  587  http://static-media.fxx.com/img/FX_Networks_-_...          6.3   \n",
       "510  571  http://static-media.fxx.com/img/FX_Networks_-_...          6.9   \n",
       "514  586  http://static-media.fxx.com/img/FX_Networks_-_...          6.7   \n",
       "518  591  http://static-media.fxx.com/img/FX_Networks_-_...          6.5   \n",
       "519  574  http://static-media.fxx.com/img/FX_Networks_-_...          6.9   \n",
       "520  589  http://static-media.fxx.com/img/FX_Networks_-_...          6.3   \n",
       "521  582  http://static-media.fxx.com/img/FX_Networks_-_...          7.1   \n",
       "522  577  http://static-media.fxx.com/img/FX_Networks_-_...          7.2   \n",
       "548  578  http://static-media.fxx.com/img/FX_Networks_-_...          7.5   \n",
       "549  594  http://static-media.fxx.com/img/FX_Networks_-_...          6.7   \n",
       "564  441  http://static-media.fxx.com/img/FX_Networks_-_...          7.2   \n",
       "574  592  http://static-media.fxx.com/img/FX_Networks_-_...          6.4   \n",
       "575  596  http://static-media.fxx.com/img/FX_Networks_-_...          6.9   \n",
       "576  597  http://static-media.fxx.com/img/FX_Networks_-_...          6.6   \n",
       "\n",
       "     imdb_votes  number_in_season  number_in_series original_air_date  \\\n",
       "161       757.0                 4               424        2008-11-02   \n",
       "189       543.0                 6               447        2009-11-22   \n",
       "209       705.0                20               550        2014-05-04   \n",
       "216       334.0                18               570        2015-04-19   \n",
       "229       335.0                21               573        2015-05-10   \n",
       "230       395.0                 2               576        2015-10-04   \n",
       "231       335.0                 6               580        2015-11-08   \n",
       "232       698.0                 9               583        2015-12-13   \n",
       "233       309.0                11               585        2016-01-10   \n",
       "234         NaN                 2               598        2016-10-02   \n",
       "235         NaN                 3               599        2016-10-09   \n",
       "236         NaN                 4               600        2016-10-16   \n",
       "237       293.0                14               588        2016-02-21   \n",
       "238       262.0                16               590        2016-03-13   \n",
       "239       229.0                19               593        2016-04-24   \n",
       "240       227.0                21               595        2016-05-15   \n",
       "501       385.0                 5               579        2015-10-25   \n",
       "502       356.0                10               584        2016-01-03   \n",
       "504       532.0                 1               575        2015-09-27   \n",
       "505       330.0                20               572        2015-05-03   \n",
       "506       374.0                17               569        2015-03-15   \n",
       "507       328.0                 7               581        2015-11-22   \n",
       "509       252.0                13               587        2016-02-14   \n",
       "510       347.0                19               571        2015-04-26   \n",
       "514       271.0                12               586        2016-01-17   \n",
       "518       246.0                17               591        2016-04-03   \n",
       "519       335.0                22               574        2015-05-17   \n",
       "520       233.0                15               589        2016-03-06   \n",
       "521       305.0                 8               582        2015-12-06   \n",
       "522       404.0                 3               577        2015-10-11   \n",
       "548       462.0                 4               578        2015-10-18   \n",
       "549       193.0                20               594        2016-05-08   \n",
       "564       648.0                21               441        2009-05-17   \n",
       "574       228.0                18               592        2016-04-10   \n",
       "575       210.0                22               596        2016-05-22   \n",
       "576       104.0                 1               597        2016-09-25   \n",
       "\n",
       "     original_air_year production_code  season  \\\n",
       "161               2008          KABF16      20   \n",
       "189               2009          LABF18      21   \n",
       "209               2014          RABF21      25   \n",
       "216               2015          TABF11      26   \n",
       "229               2015          TABF15      26   \n",
       "230               2015          TABF17      27   \n",
       "231               2015          TABF21      27   \n",
       "232               2015          VABF02      27   \n",
       "233               2016          VABF04      27   \n",
       "234               2016          VABF18      28   \n",
       "235               2016          VABF17      28   \n",
       "236               2016          VABF16      28   \n",
       "237               2016          VABF06      27   \n",
       "238               2016          VABF09      27   \n",
       "239               2016          VABF12      27   \n",
       "240               2016          VABF13      27   \n",
       "501               2015          TABF18      27   \n",
       "502               2016          VABF03      27   \n",
       "504               2015          TABF14      27   \n",
       "505               2015          TABF13      26   \n",
       "506               2015          TABF10      26   \n",
       "507               2015          TABF20      27   \n",
       "509               2016          VABF07      27   \n",
       "510               2015          TABF12      26   \n",
       "514               2016          VABF05      27   \n",
       "518               2016          VABF10      27   \n",
       "519               2015          TABF16      26   \n",
       "520               2016          VABF08      27   \n",
       "521               2015          VABF01      27   \n",
       "522               2015          TABF19      27   \n",
       "548               2015          TABF22      27   \n",
       "549               2016          VABF14      27   \n",
       "564               2009          LABF12      20   \n",
       "574               2016          VABF11      27   \n",
       "575               2016          VABF15      27   \n",
       "576               2016          VABF20      28   \n",
       "\n",
       "                                     title  us_viewers_in_millions  \\\n",
       "161                Treehouse of Horror XIX                   12.48   \n",
       "189                      Pranks and Greens                    7.03   \n",
       "209                          Brick Like Me                    4.39   \n",
       "216                            Peeping Mom                    3.23   \n",
       "229                                 Bull-E                    2.77   \n",
       "230                          Cue Detective                    6.02   \n",
       "231                    Friend with Benefit                    3.48   \n",
       "232                               Barthood                    5.97   \n",
       "233     Teenage Mutant Milk-Caused Hurdles                    8.33   \n",
       "234               Friends and Family\"[203]                     NaN   \n",
       "235                         The Town\"[205]                     NaN   \n",
       "236        Treehouse of Horror XXVII\"[207]                     NaN   \n",
       "237                 Gal of Constant Sorrow                    3.10   \n",
       "238               The Marge-ian Chronicles                    3.07   \n",
       "239                           Fland Canyon                    2.77   \n",
       "240                            Simprovised                    2.80   \n",
       "501               Treehouse of Horror XXVI                    6.75   \n",
       "502                          The Girl Code                    4.41   \n",
       "504                      Every Man's Dream                    3.28   \n",
       "505                    Let's Go Fly a Coot                    3.12   \n",
       "506                    Waiting for Duffman                    3.59   \n",
       "507                       Lisa with an 'S'                    5.64   \n",
       "509  Love Is in the N2-O2-Ar-CO2-Ne-He-CH4                    2.89   \n",
       "510                 The Kids Are All Fight                    3.33   \n",
       "514               Much Apu About Something                    3.95   \n",
       "518                         The Burns Cage                    2.32   \n",
       "519                        Mathlete's Feat                    2.82   \n",
       "520                  Lisa the Veterinarian                    3.09   \n",
       "521                         Paths of Glory                    5.53   \n",
       "522                               Puffless                    3.31   \n",
       "548                    Halloween of Horror                    3.69   \n",
       "549                   To Courier with Love                    2.52   \n",
       "564                     Coming to Homerica                    5.86   \n",
       "574            How Lisa Got Her Marge Back                    2.55   \n",
       "575               Orange Is the New Yellow                    2.54   \n",
       "576            Monty Burns' Fleeing Circus                    3.36   \n",
       "\n",
       "                                           video_url    views  \n",
       "161  http://www.simpsonsworld.com/video/691895363987  21509.0  \n",
       "189                                              NaN      NaN  \n",
       "209  http://www.simpsonsworld.com/video/311243331763  65613.0  \n",
       "216  http://www.simpsonsworld.com/video/430426691868  40157.0  \n",
       "229  http://www.simpsonsworld.com/video/442879555692  43978.0  \n",
       "230  http://www.simpsonsworld.com/video/770595395906    178.0  \n",
       "231  http://www.simpsonsworld.com/video/770437187884    175.0  \n",
       "232  http://www.simpsonsworld.com/video/770423875541    182.0  \n",
       "233  http://www.simpsonsworld.com/video/770430019813    172.0  \n",
       "234                                              NaN      NaN  \n",
       "235                                              NaN      NaN  \n",
       "236                                              NaN      NaN  \n",
       "237  http://www.simpsonsworld.com/video/770451523845    144.0  \n",
       "238  http://www.simpsonsworld.com/video/770891331927    201.0  \n",
       "239  http://www.simpsonsworld.com/video/770894403718    192.0  \n",
       "240  http://www.simpsonsworld.com/video/770892355976    228.0  \n",
       "501  http://www.simpsonsworld.com/video/770433091920    195.0  \n",
       "502  http://www.simpsonsworld.com/video/770434627559    166.0  \n",
       "504  http://www.simpsonsworld.com/video/770426947744    187.0  \n",
       "505  http://www.simpsonsworld.com/video/438408259807  40783.0  \n",
       "506  http://www.simpsonsworld.com/video/420981315819  37874.0  \n",
       "507  http://www.simpsonsworld.com/video/770417219938    145.0  \n",
       "509  http://www.simpsonsworld.com/video/770439747712    158.0  \n",
       "510  http://www.simpsonsworld.com/video/434682947703  42123.0  \n",
       "514  http://www.simpsonsworld.com/video/770449475567    170.0  \n",
       "518  http://www.simpsonsworld.com/video/770893891557    190.0  \n",
       "519  http://www.simpsonsworld.com/video/446649411760  47429.0  \n",
       "520  http://www.simpsonsworld.com/video/770452547696    174.0  \n",
       "521  http://www.simpsonsworld.com/video/770422339536    170.0  \n",
       "522  http://www.simpsonsworld.com/video/772103747997    160.0  \n",
       "548  http://www.simpsonsworld.com/video/770425923947    196.0  \n",
       "549  http://www.simpsonsworld.com/video/772102211921    192.0  \n",
       "564  http://www.simpsonsworld.com/video/729848899646   6477.0  \n",
       "574  http://www.simpsonsworld.com/video/770893891750    188.0  \n",
       "575  http://www.simpsonsworld.com/video/772111939897    276.0  \n",
       "576  http://www.simpsonsworld.com/video/772654659902    994.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes[~df_episodes.id.isin(df.episode_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Set A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our base datasets, we can do some time series analyses on the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise A1: What are the minimum, maximum and mean number of script lines per episode?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find it helpful to use the following `pandas` functions:\n",
    "* [`pandas.DataFrame.groupby()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html)\n",
    "* [`pandas.Series.nunique()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.nunique.html)\n",
    "* [`pandas.Series.min()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.min.html), [`pandas.Series.max()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.max.html), [`pandas.Series.mean()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.mean.html) .. and/or [`pandas.core.groupby.DataFrameGroupBy.agg()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.agg.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max lines: 334.0\n",
      "min lines: 78.0\n",
      "mean lines: 234.20035461\n",
      "mean lines: 232.0\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "grouped = df.groupby(by='episode_id').speaking_line.sum()\n",
    "print('max lines: ' + str(grouped.max()))\n",
    "print('min lines: ' + str(grouped.min()))\n",
    "print('mean lines: ' + str(grouped.mean()))\n",
    "print('mean lines: ' + str(grouped.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise A2: Show (plot) the distribution of the number of script lines per episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find it helpful to use the [`pandas.DataFrame.hist()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.hist.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x114029890>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113c9ed10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "grouped.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise A3: What are the minimum, maximum and mean number of episodes per season?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max episodes: 25\n",
      "min episodes: 13\n",
      "mean episodes: 21.6923076923\n",
      "median episodes: 22.0\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "seasoned = df.groupby(by='season').episode_id.nunique()\n",
    "print('max episodes: ' + str(seasoned.max()))\n",
    "print('min episodes: ' + str(seasoned.min()))\n",
    "print('mean episodes: ' + str(seasoned.mean()))\n",
    "print('median episodes: ' + str(seasoned.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise A4: Show (plot) the distribution of the number of episodes per season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x113ed94d0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112a65e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "seasoned.hist(bins= 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise A5: What are the minimum, maximun and mean number of characters appearing per episode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise A6: Show (plot) the distribution of the number of characters per episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Set B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the interesting questions we might ask about The Simpsons are focused on the characters.\n",
    "\n",
    "In order to simpify analysis of The Simpsons characters, it will be useful to construct a new `DataFrame` containing data associated with each character (vs. data associated with each episode or script line, although we will use data from both sources in constructing the new `DataFrame`).\n",
    "\n",
    "The following exercises will focus primarily on using some standard `pandas` functions, and set the stage for doing some additional time series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise B1: Determine the number of lines associated with each character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(We will do this together)_\n",
    "\n",
    "First, it is important to note that we have two fields in the `DataFrame` extracted from `simpsons_script_lilnes.csv` that are associated with characters:\n",
    "* `character_id`: a unique identifier for each character\n",
    "* `raw_character_text`: the full text (name) associated in the script with each appearance of a character\n",
    "\n",
    "And it is further important to note that there are more distinct `raw_character_text` values than there are distinct `character_id` values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6764, 6720)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.raw_character_text.nunique(), df.character_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we group the script lines by `character_id` (using [`pandas.DataFrame.groupby()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html)) and then gather the distinct `raw_character_text` values associated with each `character_id` (using [`pandas.Series.unique()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html)), the first few rows indicate why there are more `raw_character_text` values than `character_id` values.\n",
    "\n",
    "A given character can have different personae within the context of a show (e.g., \"Marge Simpson\" and \"Young Marge\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "character_id\n",
       "0                                                   []\n",
       "1    [Marge Simpson, Young Marge, Marge's Thoughts,...\n",
       "2    [Homer Simpson, Homer's Brain, Young Homer, Te...\n",
       "3                                    [Seymour Skinner]\n",
       "4                                              [JANEY]\n",
       "Name: raw_character_text, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('character_id').raw_character_text.unique().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we group by both `character_id` and `raw_character_text`, and then find the number of rows in each group (using [`pandas.core.groupby.GroupBy.size()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.size.html)), and then convert the resulting [`MultiIndex`](https://pandas.pydata.org/pandas-docs/stable/advanced.html) object back into a `DataFrame` (using [`pandas.DataFrame.reset_index()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html)), we can create a new DataFrame with a row for each `raw_character_text` value, wherein each row has \n",
    "* `character_id`: a non-unique value which may be shared across other `raw_character_text` rows\n",
    "* `raw_character_text`: a unique full text (name) for the character\n",
    "* a column indicating the number of script lines associated with that `raw_character_text` (labeled `0`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>17521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Marge Simpson</td>\n",
       "      <td>14157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Marge's Thoughts</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Teenage Marge</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Young Marge</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>29839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>Homer's Brain</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>Homer's Thoughts</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>Teenage Homer</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>Young Homer</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   character_id raw_character_text      0\n",
       "0             0                     17521\n",
       "1             1      Marge Simpson  14157\n",
       "2             1   Marge's Thoughts     23\n",
       "3             1      Teenage Marge     11\n",
       "4             1        Young Marge     72\n",
       "5             2      Homer Simpson  29839\n",
       "6             2      Homer's Brain     93\n",
       "7             2   Homer's Thoughts     44\n",
       "8             2      Teenage Homer     34\n",
       "9             2        Young Homer     95"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['character_id', 'raw_character_text']).size().reset_index().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could use [`pandas.Series.nunique()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.nunique.html) to count the number of unique `id` values within each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>17521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Marge Simpson</td>\n",
       "      <td>14157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Marge's Thoughts</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Teenage Marge</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Young Marge</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>29839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>Homer's Brain</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>Homer's Thoughts</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>Teenage Homer</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>Young Homer</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   character_id raw_character_text     id\n",
       "0             0                     17521\n",
       "1             1      Marge Simpson  14157\n",
       "2             1   Marge's Thoughts     23\n",
       "3             1      Teenage Marge     11\n",
       "4             1        Young Marge     72\n",
       "5             2      Homer Simpson  29839\n",
       "6             2      Homer's Brain     93\n",
       "7             2   Homer's Thoughts     44\n",
       "8             2      Teenage Homer     34\n",
       "9             2        Young Homer     95"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['character_id', 'raw_character_text'])['id'].nunique().reset_index().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rename the column showing the number of script lines (using [`pandas.DataFrame.rename()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html)), and assign the resulting `DataFrame` to a new variable, `df_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_characters = df.groupby(['character_id', 'raw_character_text'])['id'].nunique().reset_index().rename(\n",
    "    columns={'id': 'num_lines'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>num_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>17521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Marge Simpson</td>\n",
       "      <td>14157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Marge's Thoughts</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Teenage Marge</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Young Marge</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>29839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>Homer's Brain</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>Homer's Thoughts</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>Teenage Homer</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>Young Homer</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   character_id raw_character_text  num_lines\n",
       "0             0                         17521\n",
       "1             1      Marge Simpson      14157\n",
       "2             1   Marge's Thoughts         23\n",
       "3             1      Teenage Marge         11\n",
       "4             1        Young Marge         72\n",
       "5             2      Homer Simpson      29839\n",
       "6             2      Homer's Brain         93\n",
       "7             2   Homer's Thoughts         44\n",
       "8             2      Teenage Homer         34\n",
       "9             2        Young Homer         95"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_characters.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort the characters based on the number of lines they have (using [`pandas.DataFrame.sort_values()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>num_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>29839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>17521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Marge Simpson</td>\n",
       "      <td>14157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>Bart Simpson</td>\n",
       "      <td>13776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>11500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>15</td>\n",
       "      <td>C. Montgomery Burns</td>\n",
       "      <td>3172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>17</td>\n",
       "      <td>Moe Szyslak</td>\n",
       "      <td>2863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>Seymour Skinner</td>\n",
       "      <td>2443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>11</td>\n",
       "      <td>Ned Flanders</td>\n",
       "      <td>2145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>31</td>\n",
       "      <td>Grampa Simpson</td>\n",
       "      <td>1886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>25</td>\n",
       "      <td>Milhouse Van Houten</td>\n",
       "      <td>1862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>71</td>\n",
       "      <td>Chief Wiggum</td>\n",
       "      <td>1836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>139</td>\n",
       "      <td>Krusty the Clown</td>\n",
       "      <td>1772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>101</td>\n",
       "      <td>Nelson Muntz</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>165</td>\n",
       "      <td>Lenny Leonard</td>\n",
       "      <td>1166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>208</td>\n",
       "      <td>Apu Nahasapeemapetilon</td>\n",
       "      <td>1006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>14</td>\n",
       "      <td>Waylon Smithers</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>211</td>\n",
       "      <td>Kent Brockman</td>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>170</td>\n",
       "      <td>Carl Carlson</td>\n",
       "      <td>883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>40</td>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     character_id       raw_character_text  num_lines\n",
       "5               2            Homer Simpson      29839\n",
       "0               0                               17521\n",
       "1               1            Marge Simpson      14157\n",
       "17              8             Bart Simpson      13776\n",
       "21              9             Lisa Simpson      11500\n",
       "29             15      C. Montgomery Burns       3172\n",
       "32             17              Moe Szyslak       2863\n",
       "10              3          Seymour Skinner       2443\n",
       "25             11             Ned Flanders       2145\n",
       "49             31           Grampa Simpson       1886\n",
       "42             25      Milhouse Van Houten       1862\n",
       "92             71             Chief Wiggum       1836\n",
       "161           139         Krusty the Clown       1772\n",
       "122           101             Nelson Muntz       1174\n",
       "189           165            Lenny Leonard       1166\n",
       "232           208   Apu Nahasapeemapetilon       1006\n",
       "28             14          Waylon Smithers       1001\n",
       "236           211            Kent Brockman        894\n",
       "194           170             Carl Carlson        883\n",
       "60             40  Edna Krabappel-Flanders        745"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_characters.sort_values('num_lines', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can exclude the null character by restricting the rows to those for which `character_id>0` or those for which `raw_character_text!=''`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>num_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>29839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Marge Simpson</td>\n",
       "      <td>14157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>Bart Simpson</td>\n",
       "      <td>13776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>11500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>15</td>\n",
       "      <td>C. Montgomery Burns</td>\n",
       "      <td>3172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>17</td>\n",
       "      <td>Moe Szyslak</td>\n",
       "      <td>2863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>Seymour Skinner</td>\n",
       "      <td>2443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>11</td>\n",
       "      <td>Ned Flanders</td>\n",
       "      <td>2145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>31</td>\n",
       "      <td>Grampa Simpson</td>\n",
       "      <td>1886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>25</td>\n",
       "      <td>Milhouse Van Houten</td>\n",
       "      <td>1862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    character_id   raw_character_text  num_lines\n",
       "5              2        Homer Simpson      29839\n",
       "1              1        Marge Simpson      14157\n",
       "17             8         Bart Simpson      13776\n",
       "21             9         Lisa Simpson      11500\n",
       "29            15  C. Montgomery Burns       3172\n",
       "32            17          Moe Szyslak       2863\n",
       "10             3      Seymour Skinner       2443\n",
       "25            11         Ned Flanders       2145\n",
       "49            31       Grampa Simpson       1886\n",
       "42            25  Milhouse Van Houten       1862"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_characters[df_characters.character_id>0].sort_values('num_lines', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise B2: Determine the number of episodes in which each character appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More specifically: add a new column to `df_characters`, named `num_episodes`, containing a count of the number of `episode_id` values associated with each `raw_character_text` value.\n",
    "\n",
    "It may be useful to break this down into two steps:\n",
    "* Create a new `DataFrame` containing the number of `episode_id` values associated with each `raw_character_text` value (hint: you might use  [`pandas.Series.nunique()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.nunique.html) again)\n",
    "* [Merge](http://pandas.pydata.org/pandas-docs/stable/merging.html) that `DataFrame` with the existing `df_characters` (using [`pandas.DataFrame.merge()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise B3: Determine the first and last episodes in which each character appears"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More specifically, add the following columns to `df_characters`:\n",
    "* `first_episode_id`: the first (smallest) `episode_id` in which the associated `raw_character_text` value appears\n",
    "* `last_episode_id`: the last (largest) `episode_id` in which the associated `raw_character_text` value appears\n",
    "\n",
    "You can either follow the pattern in the previous exercise - twice - or combine the two into a single grouping (using the [`pandas.core.groupby.DataFrameGroupBy.agg()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.agg.html)) before merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise B4: Determine the number of seasons, the first season and the last season in which a character appears"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More specifically, add the following columns to `df_characters`:\n",
    "* `num_seasons`: the number of distinct `seasons` in which the associated `raw_character_text` appears\n",
    "* `first_season`: the first `season` in which the associated `raw_character_text` value appears\n",
    "* `last_episode_id`: the last `season` in which the associated `raw_character_text` value appears\n",
    "\n",
    "You should be able to modify the code you wrote for Exercises 2 and 3 above to address this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your `DataFrame` should look similar to the one below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_characters = pydata_simpsons.create_simpsons_characters_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_characters.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Set C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have `df_characters` constructed, we can do some additional analysis (more easily)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise C1: What are the minimum, maximum and mean number of script lines per character?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unevenly spaced time series analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As noted earlier (from the Wikipedia article on Time series):\n",
    "\n",
    "> Most commonly, a time series is a sequence taken at successive equally spaced points in time\n",
    "\n",
    "Unevenly spaced time series analysis involves data points that are spaced over a sequence of unequal time intervals. The sizes of the intervals themselves are often a focus of such analysis.\n",
    "\n",
    "Examples of such analyses I've done at Indeed include\n",
    "* identifying \"repeat business\" from Indeed Hire clients, based on different definitions of \"repeat\", by analyzing the intervals between the dates on which new job requisitions from the same client were created\n",
    "* determining how much time Indeed Hire recruiters spend reviewing candidate resumes and cover letters, by analyzing the intervals between candidate profile dispositions\n",
    "\n",
    "Within the context of this tutorial, we'll restrict our focus to unevenly spaced time series analysis of The Simpsons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify scene changes (an unevenly spaced time series analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One potential indicator of scene changes is changes of location.\n",
    "\n",
    "One way to identify changes of location is to create a new column with the location id of the previous script line and then look for rows in which the previous location id is different from the [current] location id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to remember that there is noise in the data, including script lines for which no location information is available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(407, 407, 407)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(df[df.location_id==0]), \n",
    " len(df[df.raw_location_text=='']), \n",
    " len(df[(df.location_id==0) & (df.raw_location_text=='')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>season</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>timestamp_in_ms</th>\n",
       "      <th>speaking_line</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>raw_location_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4576</th>\n",
       "      <td>4588</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>Marge Simpson: (CLEARING HER THROAT) Hello, ev...</td>\n",
       "      <td>5000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Marge Simpson</td>\n",
       "      <td></td>\n",
       "      <td>Hello, everyone. You know, Halloween is a very...</td>\n",
       "      <td>hello everyone you know halloween is a very st...</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12759</th>\n",
       "      <td>12786</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>Homer-Ape: (INSPIRED) Oooh.</td>\n",
       "      <td>37000</td>\n",
       "      <td>True</td>\n",
       "      <td>625</td>\n",
       "      <td>0</td>\n",
       "      <td>Homer-Ape</td>\n",
       "      <td></td>\n",
       "      <td>Oooh.</td>\n",
       "      <td>oooh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12760</th>\n",
       "      <td>12787</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>Homer-Ape: (ADJUSTING HIMSELF) Ahhh.</td>\n",
       "      <td>95000</td>\n",
       "      <td>True</td>\n",
       "      <td>625</td>\n",
       "      <td>0</td>\n",
       "      <td>Homer-Ape</td>\n",
       "      <td></td>\n",
       "      <td>Ahhh.</td>\n",
       "      <td>ahhh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14586</th>\n",
       "      <td>14613</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>Bret: Hey everybody, Bret Gunsilman here in pi...</td>\n",
       "      <td>35000</td>\n",
       "      <td>True</td>\n",
       "      <td>704</td>\n",
       "      <td>0</td>\n",
       "      <td>Bret</td>\n",
       "      <td></td>\n",
       "      <td>Hey everybody, Bret Gunsilman here in pivotal ...</td>\n",
       "      <td>hey everybody bret gunsilman here in pivotal w...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15222</th>\n",
       "      <td>15249</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Teenager: Hey, what gives?</td>\n",
       "      <td>30000</td>\n",
       "      <td>True</td>\n",
       "      <td>628</td>\n",
       "      <td>0</td>\n",
       "      <td>Teenager</td>\n",
       "      <td></td>\n",
       "      <td>Hey, what gives?</td>\n",
       "      <td>hey what gives</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  season  episode_id  number  \\\n",
       "4576    4588       2          16       0   \n",
       "12759  12786       3          43       0   \n",
       "12760  12787       3          43       1   \n",
       "14586  14613       3          49       0   \n",
       "15222  15249       3          51       0   \n",
       "\n",
       "                                                raw_text  timestamp_in_ms  \\\n",
       "4576   Marge Simpson: (CLEARING HER THROAT) Hello, ev...             5000   \n",
       "12759                        Homer-Ape: (INSPIRED) Oooh.            37000   \n",
       "12760               Homer-Ape: (ADJUSTING HIMSELF) Ahhh.            95000   \n",
       "14586  Bret: Hey everybody, Bret Gunsilman here in pi...            35000   \n",
       "15222                         Teenager: Hey, what gives?            30000   \n",
       "\n",
       "      speaking_line  character_id  location_id raw_character_text  \\\n",
       "4576           True             1            0      Marge Simpson   \n",
       "12759          True           625            0          Homer-Ape   \n",
       "12760          True           625            0          Homer-Ape   \n",
       "14586          True           704            0               Bret   \n",
       "15222          True           628            0           Teenager   \n",
       "\n",
       "      raw_location_text                                       spoken_words  \\\n",
       "4576                     Hello, everyone. You know, Halloween is a very...   \n",
       "12759                                                                Oooh.   \n",
       "12760                                                                Ahhh.   \n",
       "14586                    Hey everybody, Bret Gunsilman here in pivotal ...   \n",
       "15222                                                     Hey, what gives?   \n",
       "\n",
       "                                         normalized_text  word_count  \n",
       "4576   hello everyone you know halloween is a very st...          83  \n",
       "12759                                               oooh           1  \n",
       "12760                                               ahhh           1  \n",
       "14586  hey everybody bret gunsilman here in pivotal w...          14  \n",
       "15222                                     hey what gives           3  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.location_id==0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with character data, some `location_id` values are associated with more than one `raw_location_text` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4460, 4499, 4492)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.location_id.nunique(), \n",
    " df.raw_location_text.nunique(), \n",
    " df.raw_location_text.str.lower().nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Simpson Home                       35056\n",
       "Springfield Elementary School       7092\n",
       "Moe's Tavern                        4626\n",
       "Springfield Nuclear Power Plant     3593\n",
       "Kwik-E-Mart                         1476\n",
       "Name: raw_location_text, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.location_id>0].raw_location_text.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DUMP                 1\n",
       "MISTY FOREST         1\n",
       "Vermont              1\n",
       "PLYMOUTH, ENGLAND    1\n",
       "SPRINGFIELD - DAY    1\n",
       "Name: raw_location_text, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.location_id>0].raw_location_text.value_counts().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of this tutorial, we will simply forge ahead, noting that the results below are, at best, approximate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Set D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise D1: Add a column, `prev_location_id`, to contain the previous  `location_id` for each script line within a given season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`pandas.DataFrame.shift()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.shift.html) function can be used to return the rows of a `DataFrame` shifted by some offset (the default offset is 1 row). Positive offsets can be interpreted as shifting \"down\", which allows us to easily access previous rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    2\n",
       "3    2\n",
       "4    2\n",
       "Name: location_id, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.location_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    NaN\n",
       "1    1.0\n",
       "2    2.0\n",
       "3    2.0\n",
       "4    2.0\n",
       "Name: location_id, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.location_id.shift().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use negative offset values to shift \"up\" for next rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2.0\n",
       "1    2.0\n",
       "2    2.0\n",
       "3    2.0\n",
       "4    3.0\n",
       "Name: location_id, dtype: float64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.location_id.shift(-1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to first group by episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    NaN\n",
       "1    1.0\n",
       "2    2.0\n",
       "3    2.0\n",
       "4    2.0\n",
       "Name: location_id, dtype: float64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['episode_id']).location_id.shift().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to keep things consistent with the `location_id` field, we can substitute zero for null (`NaN`) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    2\n",
       "3    2\n",
       "4    2\n",
       "Name: location_id, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('episode_id').location_id.shift().fillna(0).astype('int64').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[:, 'prev_location_id'] = df.groupby('episode_id').location_id.shift().fillna(0).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>prev_location_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   location_id  prev_location_id\n",
       "0            1                 0\n",
       "1            2                 1\n",
       "2            2                 2\n",
       "3            2                 2\n",
       "4            2                 2"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['location_id', 'prev_location_id']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise D2: How many scene changes are there across all episodes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we conveniently ignore the fact that we have rows with missing locations (where `location_id==0`), we can compute this by simply counting the number of rows where the `location_id` and `prev_location_id` differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17122"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df.location_id != df.prev_location_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most script lines take place within the same scene (location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141125"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df.location_id == df.prev_location_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
